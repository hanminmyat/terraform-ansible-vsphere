---
- hosts: all
  become: true
  vars:
    spark_archive_path: "/home/hanmin/Downloads/spark-3.3.4-bin-hadoop3.tgz"

  tasks:
    - name: Add nameserver entry to /etc/resolv.conf
      ansible.builtin.lineinfile:
        path: /etc/resolv.conf
        line: "nameserver 8.8.8.8"

    - name: Update package lists
      yum:
        name: "*"
        state: latest

    - name: Install required packages
      yum:
        name:
          - wget
          - unzip
          - openssl-devel
          - gcc
          - make
          - python3 # For `ansible.builtin.systemd` module
          - ca-certificates
        state: present

    - name: Install Java
      yum:
        name: java-devel
        state: present

    - name: delete existing spark dir for no error
      command: "rm -rf /opt/spark"

    - name: Copy Spark archive to remote
      ansible.builtin.copy:
        src: "{{ spark_archive_path }}"
        dest: "/opt/spark-3.3.4-bin-hadoop3.tgz"

    - name: Extract Spark and create directory if not exists
      ansible.builtin.command:
        cmd: "tar zxvf /opt/spark-3.3.4-bin-hadoop3.tgz -C /opt/"
        creates: "/opt/spark-3.3.4-bin-hadoop3"
      args:
        creates: "/opt/spark-3.3.4-bin-hadoop3"
      become: true

    # - name: Create Spark directory under /opt
    #   ansible.builtin.file:
    #     path: "/opt/spark"
    #     state: directory
    #     mode: '0755'
    #   become: true

    - name: Move Spark files
      ansible.builtin.command:
        cmd: "mv /opt/spark-3.3.4-bin-hadoop3/ /opt/spark"
      args:
        chdir: "/opt/"
      become: true

    - name: Change Ownership of Spark file
      command:
        cmd: "chown -R root:root /opt/spark"

    - name: Add Path 
      lineinfile:
        path: ~/.bashrc
        line: |
          export SPARK_HOME=/opt/spark
          export PATH=$PATH:$SPARK_HOME/bin
        insertafter: EOF

    - name: Source ~/.bashrc
      shell: |
        source /root/.bashrc
      args:
        executable: /bin/bash

    - name: Echo SPARK_MASTER_HOST to spark-env.sh
      command: echo "SPARK_MASTER_HOST=10.10.10.30" > /opt/spark/conf/spark-env.sh

    - name: Add slave node
      command: echo "10.10.10.31" > /opt/spark/conf/workers
      when: inventory_hostname in groups["spark-master"]

    - name: Stop SPARK_Mater if running
      command: /opt/spark/sbin/stop-master.sh
      when: inventory_hostname in groups["spark-master"]

    - name: Stop Workers if running
      command: "/opt/spark/sbin/stop-worker.sh"
      when: inventory_hostname in groups["spark-workers"]

    - name: Start SPARK_Mater
      command: /opt/spark/sbin/start-master.sh
      when: inventory_hostname in groups["spark-master"]

    - name: Start Workers
      command: "/opt/spark/sbin/start-worker.sh spark://{{ groups['spark-master'][0] }}:7077"
      when: inventory_hostname in groups["spark-workers"]
    
    - name: Enable and start firewalld
      systemd:
        name: firewalld
        enabled: yes
        state: started

    - name: Add firewall rules for Spark
      firewalld:
        zone: public
        permanent: yes
        state: enabled
        port: "{{ item }}"
      loop:
        - 4040/tcp
        - 6066/tcp
        - 7077/tcp
        - 8080-8081/tcp

    - name: Reload firewalld
      systemd:
        name: firewalld
        state: reloaded

    # - name: RUN simple job to Spark
    #   command: pyspark --master spark://{{ groups['spark-master'][0] }}:7077
    #   when: inventory_hostname in groups["spark-master"]
